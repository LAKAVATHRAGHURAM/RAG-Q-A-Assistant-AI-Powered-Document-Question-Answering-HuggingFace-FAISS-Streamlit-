# ğŸ“˜ RAG Q&A Assistant â€“ AI-Powered Document Question Answering  
### (HuggingFace â€¢ FAISS â€¢ Streamlit â€¢ Sentence Transformers)

A lightweight and powerful **Retrieval-Augmented Generation (RAG)** application that allows users to upload **documents, text, or URLs**, and ask questions about them.  
The system extracts relevant information using **FAISS vector search** and generates **accurate, context-aware answers** using **Llama-3 (8B)** via the HuggingFace Inference API.

---

## ğŸš€ Features

- **Multi-format input support** â€” Text, PDF, DOCX, TXT, URL  
- **Fast vector search** powered by FAISS  
- **High-quality embeddings** using `sentence-transformers/all-mpnet-base-v2`  
- **AI responses** generated by *Meta-Llama-3-8B-Instruct*  
- Clean & simple **Streamlit UI**  
- Accurate, context-aware answers using the **RAG pipeline**

---

## ğŸ›  Tech Stack

| Component | Technology |
|----------|------------|
| Embeddings | Sentence Transformers |
| Vector Store | FAISS |
| LLM | Meta-Llama-3-8B-Instruct (HuggingFace API) |
| UI | Streamlit |
| Document Processing | PyPDF2, python-docx |
| Framework | LangChain Community |

---

## ğŸ“‚ Project Structure


â”œâ”€â”€ app.py # Main Streamlit application

â”œâ”€â”€ requirements.txt # Dependencies

â””â”€â”€ README.md # Documentation



---

## ğŸ”§ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/LAKAVATHRAGHURAM/RAG-Q-A-Assistant-AI-Powered-Document-Question-Answering-HuggingFace-FAISS-Streamlit-.git
```

2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```


3ï¸âƒ£ Add Your HuggingFace Token
```bash
Inside app.py
```
```bash
HF_TOKEN = "your_huggingface_token"

```
Or set it as an environment variable:
```bash
export HF_TOKEN="your_huggingface_token"
```


â–¶ï¸ Running the App

Launch Streamlit:
```bash
streamlit run app.py
```

Your app will open automatically at:
```bash
http://localhost:8501
```

## ğŸ§  How It Works

User uploads a document or enters a URL/text

System extracts text from the source

Text is split into overlapping chunks

Embeddings are generated for each chunk

FAISS index stores embeddings for fast similarity search

User asks a question â†’ relevant chunks are retrieved

Llama-3 model generates the final answer using the retrieved context

## ğŸ“Œ Example Use Cases

Chat with any PDF, DOCX, or research paper

AI-powered study and revision assistant

Technical documentation helper

Research assistant

Knowledge-base Q&A system

## âš ï¸ Limitations

Llama-3-8B via API may truncate long responses

FAISS search is CPU-only (slower for very large datasets)

Web page extraction depends on the site's HTML structure

## ğŸ¤ Contributing

Contributions are welcome!
You may improve the app by adding:

Multi-file upload support

Conversation history

Citations in answers

Advanced UI features

Support for multiple LLM providers
